{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Coarse to Fine\n",
    "- undertake the first part of a Coarse to Fine search. This involves analyzing the results of an initial random search that took place over a large search space, then deciding what would be the next logical step to make your hyperparameter search finer.\n",
    "\n",
    "We have available:\n",
    "\n",
    "- combinations_list - a list of the possible hyperparameter combinations the random search was undertaken on.\n",
    "- results_df - a DataFrame that has each hyperparameter combination and the resulting accuracy of all 500 trials. Each hyperparameter is a column, with the header the hyperparameter name.\n",
    "- visualize_hyperparameter() - a function that takes in a column of the DataFrame (as a string) and produces a scatter plot of this column's values compared to the accuracy scores. An example call of the function would be visualize_hyperparameter('accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_hyperparameter(name):\n",
    "    plt.clf()\n",
    "    plt.scatter(results_df[name],results_df['accuracy'], c=['blue']*500)\n",
    "    plt.gca().set(xlabel='{}'.format(name), ylabel='accuracy', title='Accuracy for different {}s'.format(name))\n",
    "    plt.gca().set_ylim([0,100])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm the size of the combinations_list\n",
    "print(len(combinations_list))\n",
    "\n",
    "# Sort the results_df by accuracy and print the top 10 rows\n",
    "print(results_df.sort_values(by='accuracy', ascending=False).head(10))\n",
    "\n",
    "# Confirm which hyperparameters were used in this search\n",
    "print(results_df.columns)\n",
    "\n",
    "# Call visualize_hyperparameter() with each hyperparameter in turn\n",
    "visualize_hyperparameter('max_depth')\n",
    "visualize_hyperparameter('min_samples_leaf')\n",
    "visualize_hyperparameter('learn_rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coarse to Fine Iterations\n",
    "- now visualize the first random search undertaken, construct a tighter grid and check the results. We will have available:\n",
    "\n",
    "- results_df - a DataFrame that has the hyperparameter combination and the resulting accuracy of all 500 trials. Only the hyperparameters that had the strongest visualizations are included (max_depth and learn_rate)\n",
    "- visualize_first() - This function takes no arguments but will visualize each of our hyperparameters against accuracy for our first random search.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_first():\n",
    "    for name in results_df.columns[0:2]:\n",
    "        plt.clf()\n",
    "        plt.scatter(results_df[name],results_df['accuracy'], c=['blue']*500)\n",
    "        plt.gca().set(xlabel='{}'.format(name), ylabel='accuracy', title='Accuracy for different {}s'.format(name))\n",
    "        plt.gca().set_ylim([0,100])\n",
    "        x_line = 20\n",
    "        if name == \"learn_rate\":\n",
    "            x_line = 1\n",
    "            plt.axvline(x=x_line, color=\"red\", linewidth=4)\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use the visualize_first() function to check the values of max_depth and learn_rate that tend to perform better. A convenient red line will be added to make this explicit.\n",
    "- Now create a more narrow grid search, testing for max_depth values between 1 and 20 and for 50 learning rates between 0.001 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_second():\n",
    "    for name in results_df2.columns[0:2]:\n",
    "        plt.clf()\n",
    "        plt.scatter(results_df2[name],results_df2['accuracy'], c=['blue']*1000)\n",
    "        plt.gca().set(xlabel='{}'.format(name), ylabel='accuracy', title='Accuracy for different {}s'.format(name))\n",
    "        plt.gca().set_ylim([0,100])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the provided function to visualize the first results\n",
    "# visualize_first()\n",
    "\n",
    "# Create some combinations lists & combine:\n",
    "max_depth_list = list(range(1,21))\n",
    "learn_rate_list = np.linspace(0.001,1,50)\n",
    "\n",
    "# Call the function to visualize the second results\n",
    "visualize_second()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayes Rule in Python\n",
    "- undertake a practical example of setting up Bayes formula, obtaining new evidence and updating our 'beliefs' in order to get a more accurate result. The example will relate to the likelihood that someone will close their account for our online software product.\n",
    "- These are the probabilities we know:\n",
    "\n",
    "- 7% (0.07) of people are likely to close their account next month\n",
    "- 15% (0.15) of people with accounts are unhappy with your product (we don't know who though!)\n",
    "- 35% (0.35) of people who are likely to close their account are unhappy with our product\n",
    "\n",
    "\n",
    "- Assign the different probabilities (as decimals) to variables. p_unhappy is the likelihood someone is unhappy, p_unhappy_close is the probability that someone is unhappy with the product, given they are going to close their account.\n",
    "- Assign the probability that someone will close their account next month to the variable p_close as a decimal.\n",
    "- We interview one of our customers and discover they are unhappy. What is the probability they will close their account, now that we know this evidence? Assign the result to p_close_unhappy and print it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16333333333333336\n"
     ]
    }
   ],
   "source": [
    "# Assign probabilities to variables \n",
    "p_unhappy = 0.15\n",
    "p_unhappy_close = 0.35\n",
    "\n",
    "# Probabiliy someone will close\n",
    "p_close = 0.07\n",
    "\n",
    "# Probability unhappy person will close\n",
    "p_close_unhappy = (p_unhappy_close * p_close) /p_unhappy\n",
    "print(p_close_unhappy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We correctly were able to frame this problem in a Bayesian way, and update our beliefs using new evidence. There's a 16.3% chance that a customer, given that they are unhappy, will close their account"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Hyperparameter tuning with Hyperopt\n",
    "-  set up and run a bayesian hyperparameter optimization process using the package Hyperopt.We will set up the domain (which is similar to setting up the grid for a grid search), then set up the objective function. Finally, we will run the optimizer over 20 iterations.\n",
    "\n",
    "\n",
    "- We will need to set up the domain using values:\n",
    "\n",
    "\n",
    "- `max_depth` using quniform distribution (between 2 and 10, increasing by 2)\n",
    "- `learning_rate` using uniform distribution (0.001 to 0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from hyperopt import hp, fmin, tpe\n",
    "import numpy as np\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "data = load_iris()\n",
    "X = data['data']\n",
    "y = data['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████| 20/20 [00:07<00:00,  2.56trial/s, best loss: 0.0535714285714286]\n",
      "{'learning_rate': 0.7495361970511488, 'max_depth': 6.0}\n"
     ]
    }
   ],
   "source": [
    "# Set up space dictionary with specified hyperparameters\n",
    "space = {'max_depth': hp.quniform('max_depth', 2, 10, 2),'learning_rate': hp.uniform('learning_rate', 0.001,0.9)}\n",
    "\n",
    "# Set up objective function\n",
    "def objective(params):\n",
    "    params = {'max_depth': int(params['max_depth']),'learning_rate': params['learning_rate']}\n",
    "    gbm_clf = GradientBoostingClassifier(n_estimators=100, **params) \n",
    "    best_score = cross_val_score(gbm_clf, X_train, y_train, scoring='accuracy', cv=2, n_jobs=4).mean()\n",
    "    loss = 1 - best_score\n",
    "    return loss\n",
    "\n",
    "# Run the algorithm\n",
    "best = fmin(fn=objective,space=space, max_evals=20, rstate=np.random.RandomState(42), algo=tpe.suggest)\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Genetic Hyperparameter Tuning with TPOT\n",
    "- example of genetic hyperparameter tuning. TPOT is a very powerful library that has a lot of features. We're just scratching the surface \n",
    "- In real life, TPOT is designed to be run for many hours to find the best model. We would have a much larger population and offspring size as well as hundreds more generations to find a good model.\n",
    "- We will create the estimator, fit the estimator to the training data and then score this on the test data.\n",
    "\n",
    "- For this example we wish to use:\n",
    "\n",
    "- 3 generations\n",
    "- 4 in the population size\n",
    "- 3 offspring in each generation\n",
    "- accuracy for scoring\n",
    "\n",
    "- A random_state of 2 has been set for consistency of results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tpot import TPOTClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Optimization Progress', max=13.0, style=ProgressStyle(des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generation 1 - Current best internal CV score: 0.9642857142857143\n",
      "Generation 2 - Current best internal CV score: 0.9642857142857143\n",
      "Generation 3 - Current best internal CV score: 0.9642857142857143\n",
      "Best pipeline: GaussianNB(VarianceThreshold(input_matrix, threshold=0.0001))\n",
      "0.9736842105263158\n"
     ]
    }
   ],
   "source": [
    "# Assign the values outlined to the inputs\n",
    "number_generations = 3\n",
    "population_size = 4\n",
    "offspring_size = 3\n",
    "scoring_function = 'accuracy'\n",
    "\n",
    "# Create the tpot classifier\n",
    "tpot_clf = TPOTClassifier(generations=number_generations, population_size=population_size,\n",
    "                          offspring_size=offspring_size, scoring=scoring_function,\n",
    "                          verbosity=2, random_state=2, cv=2)\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "tpot_clf.fit(X_train, y_train)\n",
    "\n",
    "# Score on the test set\n",
    "print(tpot_clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can see in the output the score produced by the chosen model over each generation, and then the final accuracy score with the hyperparameters chosen for the final model. This is a great first example of using TPOT for automated hyperparameter tuning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
